[Files]
path = /corpus/funP/current_model/peoplenews/o/UserMoreThan5/click/nounk/balance/
train_file = train_data.h5
#neg_train_data = /corpus/funP/current_model/peoplenews/o/data_train_neg
dev_file = dev_data.h5
test_file = test_data.h5
#nb_data = /corpus/funP/current_model/peoplenews/o/UserMoreThan5/nb_data.json
#neg_nb_data = /corpus/funP/current_model/peoplenews/n/nb_data_neg
rusers_path = r_users
uaids_path = u_aids
user_dic = PPN_ucnb_uDic.json
#neg_user_dic = /corpus/funP/current_model/peoplenews/n/user_dic_neg
#topic_dic = /corpus/funP/current_model/peoplenews/peoplenews_topic_dic.json 
aid_topic_dic = /corpus/funP/current_model/peoplenews/peoplenews_id_topic.json
aid_title_dic = /corpus/funP/current_model/peoplenews/peoplenews_id_title.json
#save_model = /corpus/funP/current_model/peoplenews/h5/model_n
#save_final_weight = /corpus/funP/current_model/peoplenews/h5/UTCNN_best_n
#save_weight = /corpus/funP/current_model/peoplenews/h5/UTCNN_itr{epoch:02d}_n.h5
#save_prediction = /corpus/funP/current_model/peoplenews/pickle/peoplenews_predict_n
#save_model = PPN_model
save_final_weight = final_weight.h5
save_epoch_weight = h5/itr{epoch:02d}_weight.h5
save_prediction = prediction.pkl
#random_pickle = /corpus/funP/current_model/peoplenews/pickle/random_n
content_file = /corpus/funP/current_model/peoplenews/peoplenews_idcontent_all.json
embedding_file = /corpus/funP/dataset/postall.vectors.char.50d.txt
input_type = character
# input_type = [character / word]

[Pars]
# Data parameters:
proportion_ratio = 0
# negative sample proportion
max_content = 2300
# max_content={'FHM':1850, 'PPN':2300}, limit max length of article, '0' if you don't want limitation
max_title = 40
# max_title={'FHM':26???, 'PPN':40}
max_topic = 3
topic_size = 100
max_rusers = 236
# max_rusers will change if data change!!!
max_uarticles = 0
# max_uarticles={'PPN':112}
# max_uarticles will change if data change!!!
max_cuser = 1
# Model parameters:
v_dim = 50
# dimension in the word embedding file
u_dim = 10
# dimension of the user vector embeddings
mini_u_dim = 5
# first dimension of the user matrix embeddings
t_dim = 10
# dimension of the topic vector embeddings
mini_t_dim = 5
# first dimension of the topic matrix embeddings
con_size = 50
# number of the convolution channels
l_size = 2
# number of labels
flength1 = 1
# window size in the first convolution filter
flength2 = 2
# window size in the second convolution filter
flength3 = 3
# window size in the third convolution filter
#rnd_base = 0.01
rnd_base = 1
# random base of the initial vector, in the range of [-rndBase, rndBase]
#lr = 0.00001
# learning rate
batch_size = 300
# batch size per training
batch_size_dev = 300
# batch size per develop
batch_size_test = 300
# batch size per testing
patience = 5
# number of patience waiting for best pars
max_epoch = 100
# maximum number of iteration
