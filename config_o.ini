[Files]
path = /corpus/funP/current_model/peoplenews/bySession/

train_file = train_data.h5
dev_file = dev_data.h5
test_file = test_data.h5

user_dic = ppn_bs_uid.json

title = /corpus/funP/peoplenews/peoplenews.title
topic = /corpus/funP/peoplenews/peoplenews.topic
content = /corpus/funP/peoplenews/peoplenews.content
 
#save_model = /corpus/funP/current_model/peoplenews/h5/model_n
#save_final_weight = /corpus/funP/current_model/peoplenews/h5/UTCNN_best_n
#save_weight = /corpus/funP/current_model/peoplenews/h5/UTCNN_itr{epoch:02d}_n.h5
#save_prediction = /corpus/funP/current_model/peoplenews/pickle/peoplenews_predict_n
#save_model = PPN_model
#save_final_weight = final_weight_CcontentConv_lr00001_500e.h5
#save_epoch_weight = h5/itr{epoch:02d}_weight_CcConv_lr00001_500e.h5
#save_prediction = prediction_CcConv_lr00001_500e.pkl
save_final_weight = fweight
save_epoch_weight = epochweight
save_prediction = predictresult
#random_pickle = /corpus/funP/current_model/peoplenews/pickle/random_n
embedding_file = /corpus/funP/dataset/postall.vectors.char.50d.txt
input_type = character
# input_type = [character / word]

[Pars]
# Data parameters:
proportion_ratio = 0
# negative sample proportion
max_content = 2300
# max_content={'FHM':1850, 'PPN':2300}, limit max length of article, '0' if you don't want limitation
max_title = 40
# max_title={'FHM':26???, 'PPN':40}
max_topic = 3
topic_size = 100
max_rusers = 236
# max_rusers = {'PPN':236}
# max_rusers will change if data change!!!
max_uarticles = 0
# max_uarticles={'PPN':112}
# max_uarticles will change if data change!!!
max_cuser = 1
# Model parameters:
v_dim = 50
# dimension in the word embedding file
u_dim = 10
# dimension of the user vector embeddings
mini_u_dim = 5
# first dimension of the user matrix embeddings
t_dim = 10
# dimension of the topic vector embeddings
mini_t_dim = 5
# first dimension of the topic matrix embeddings
con_size = 50
# number of the convolution channels
l_size = 2
# number of labels
flength1 = 1
# window size in the first convolution filter
flength2 = 2
# window size in the second convolution filter
flength3 = 3
# window size in the third convolution filter
#rnd_base = 0.01
rnd_base = 1
# random base of the initial vector, in the range of [-rndBase, rndBase]
lr = 0.00001
# learning rate
batch_size = 500
# batch size per training
batch_size_dev = 300
# batch size per develop
batch_size_test = 300
# batch size per testing
patience = 5
# number of patience waiting for best pars
max_epoch = 100 
# maximum number of iteration
